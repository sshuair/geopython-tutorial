{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute '__version___'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-a3f45cc0f7c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version___\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute '__version___'"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(torch.__version___)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "\n",
    "# CUDA?\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_set = MNIST('./data', train=True, download=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(), # ToTensor does min-max normalization. \n",
    "]), )\n",
    "\n",
    "test_set = MNIST('./data', train=False, download=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(), # ToTensor does min-max normalization. \n",
    "]), )\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader_args = dict(shuffle=True, batch_size=batch_size, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=batch_size)\n",
    "train_loader = dataloader.DataLoader(train_set, **dataloader_args)\n",
    "test_loader = dataloader.DataLoader(test_set, **dataloader_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow_multi(images):\n",
    "    f = plt.figure()\n",
    "    for n, im in enumerate(images):\n",
    "        im = np.clip(im, 0, 1)\n",
    "        f.add_subplot(1, len(images), n+1)  # this line outputs images on top of each other\n",
    "        # f.add_subplot(1, 2, n)  # this line outputs images side-by-side\n",
    "        fig = plt.imshow(im, cmap='gray', vmin=0, vmax=1)\n",
    "        fig.axes.get_xaxis().set_visible(False) # this is the worst api in the world\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA1ElEQVR4nN3QPwtBYRQG8EMU0e0uZLIw+QKXRZlMGC0GX8CglE0pk0VxPwQmE5YrJYPVIjYMlImSwXNiMOi97319AM/6O6fzh+g/Y5hr5mrRNByseAZba4D7EnlSN8wy3uAYXJOwDEw0ohKwD9mtxehqRLQBCnZr8GPkJ/Ll79y0m37GiIjiK2AQsGMYiIbryyvjmZO20U9gAIcjTg43GhfethOROToO+En6xRUlZhnSjd+I6BY7xVIRY79w4XapR9IOSTWWYSWUqE0xlH771R7UrULefm5U2pxVCt0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x113566400>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = train_set.train_data[4,:,:].numpy()\n",
    "Image.fromarray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAAA0CAYAAAC94kJjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAA39JREFUeJzt3U1y5CAMhmEzNVfIeu5/rOxzB2aRYsrjGBtskD7Z71OVTf5aEULQdNtJOecFAODvl3cAAIBvNGQAEEFDBgARNGQAEEFDBgARNGQAEEFDBgARNGQAEEFDBgARv3u+OaXkellfzjntfZ649u3F5R3TsixfOeeP7SeJq4q4+kjGVesRW+yQYe3TO4AK4upDXBPQkAFABA0ZAER0nSE/We2udyk1Hf0AIWzrnPrWYtKQr9zi07JQIt6CdB2z96Rikh9TWexV67w3Lq/eYPG4wxuy6qDXnMVbvj5qMEb/Pk/Rxtoa+am7k5ucs8v8sXjcoQ15VAFaJDvyZIkc+2ituVB8xmXdWFJK1I449zPkJ+wUPXnlT2FiK8RQKMVSEyHGtxvakFtWYO9VmqK8z/NMNOr4edd9jdfT/z3bOBTzNdvwHfJR4ZWER9jVlRjLz9wt3KcXl/ek3o6XEu/cFEqLwp2cqORzBt6HDAAippwhr1ew9Yrs+fToaGcQacX13uEoHjedjZ/X+CrWldIu+UiEGGdwf1HPwlsHdzTFs+OzowqrpqjYfCNSm6vWC9j0hrz9g1QuIogygZ6ys7/jrGbUJnGhGpeCq7l5es2b7JCPVhmP92KqiTBx92JUPH5aU78STrEWLahfuevJ7EW9o4TObEiqzS7n/O/jCu8LHZQnSErJPD9XxlG1NhW9JVem77JQnsRbd5rlnp4mUb53/fFmyn//3SOltzSaNeXx9Gb+ol7tRRiLo4s7E2RUbJGK0fuFstbH9GpqZ8245xLq7c9a8ri/ytljqS5Us/sU70MGABGmO2TVVa+IdlYKP1d2x8pXE0KDzP2QvRsfzbiNYk7ULnY4q6XahVNP13pvYbWcWNbXtIZ85b4Rs6kNtLJIuVKP9ai+vc7j93I283y0ZYzUx9GC+w3q1XZcavF4YGKMQz3ti94nZhnWkBV3xD0UY1JCftqRq7HelM/bDVm9EbfeoxlxeZwhUzNzvTW/ZpdOe/J+fNh60v8tnMU6N2yM2vA+ZAAQcXuHzKpmQ+kqqiiU/j0RYteVVezskPEIkSc7ULziBvV4B5oyouttyF/LsnzOCKTBn4OvEddPtbg8Y1oW4upFXH0U4zrqEf9JXAQAABo4QwYAETRkABBBQwYAETRkABBBQwYAETRkABBBQwYAETRkABBBQwYAEX8BPG7IW7al+uUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11357d208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5,  0,  4,  1,  9,  2,  1,  3,  1])\n"
     ]
    }
   ],
   "source": [
    "imshow_multi(train_set.train_data[0:9,:,:].numpy())\n",
    "print(train_set.train_labels[0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义神经网络\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(16*4*4, 100) #TODO: 寻找自动匹配方法\n",
    "        self.fc2 = nn.Linear(100, 50)\n",
    "        self.fc3 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        \n",
    "        :param x: 一个batch的数据，输入结构：[batch_size, image_channel, image_width, image_height]\n",
    "        :return: 前向传播结果\n",
    "        \"\"\"\n",
    "        # 第一阶段特征提取（卷积、relu、池化）\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "\n",
    "        # 第二阶段特征提取（卷积、relu、池化）\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "\n",
    "        # dropout\n",
    "        x = self.drop(x)\n",
    "\n",
    "        # flat features 多维维空间一维化，输出也就是[batch_size, image_channel * image_width * image_height]\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        # x = x.view(batch_size, -1)  不能用这种方法，因为最后一个epoch可能不是正好整除batch_size\n",
    "\n",
    "        # 全连接层（fc1，fc2，fc3）\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        feature_size = x.size()[1:]\n",
    "        num_flat = reduce(lambda m, n: m*n, feature_size, 1)\n",
    "        return num_flat\n",
    "    \n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(params=net.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sshuair/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:37: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/Users/sshuair/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:17: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/Users/sshuair/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:19: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch[1]: Batch[0/60000 (0.00%%)]  Loss: 0.730064\n",
      "Train Epoch[1]: Batch[4096/60000 (6.83%%)]  Loss: 0.550426\n",
      "Train Epoch[1]: Batch[8192/60000 (13.65%%)]  Loss: 0.453059\n",
      "Train Epoch[1]: Batch[12288/60000 (20.48%%)]  Loss: 0.422302\n",
      "Train Epoch[1]: Batch[16384/60000 (27.31%%)]  Loss: 0.600512\n",
      "Train Epoch[1]: Batch[20480/60000 (34.13%%)]  Loss: 0.305183\n",
      "Train Epoch[1]: Batch[24576/60000 (40.96%%)]  Loss: 0.50171\n",
      "Train Epoch[1]: Batch[28672/60000 (47.79%%)]  Loss: 0.617226\n",
      "Train Epoch[1]: Batch[32768/60000 (54.61%%)]  Loss: 0.383861\n",
      "Train Epoch[1]: Batch[36864/60000 (61.44%%)]  Loss: 0.487687\n",
      "Train Epoch[1]: Batch[40960/60000 (68.27%%)]  Loss: 0.304005\n",
      "Train Epoch[1]: Batch[45056/60000 (75.09%%)]  Loss: 0.485827\n",
      "Train Epoch[1]: Batch[49152/60000 (81.92%%)]  Loss: 0.432134\n",
      "Train Epoch[1]: Batch[53248/60000 (88.75%%)]  Loss: 0.498942\n",
      "Train Epoch[1]: Batch[57344/60000 (95.57%%)]  Loss: 0.540839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sshuair/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:34: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test  Epoch 1: Average loss: 0.128359, Accuracy:96.170000%\n",
      "\n",
      "Train Epoch[2]: Batch[0/60000 (0.00%%)]  Loss: 0.132041\n",
      "Train Epoch[2]: Batch[4096/60000 (6.83%%)]  Loss: 0.167896\n",
      "Train Epoch[2]: Batch[8192/60000 (13.65%%)]  Loss: 0.17117\n",
      "Train Epoch[2]: Batch[12288/60000 (20.48%%)]  Loss: 0.138968\n",
      "Train Epoch[2]: Batch[16384/60000 (27.31%%)]  Loss: 0.218223\n",
      "Train Epoch[2]: Batch[20480/60000 (34.13%%)]  Loss: 0.136549\n",
      "Train Epoch[2]: Batch[24576/60000 (40.96%%)]  Loss: 0.155736\n",
      "Train Epoch[2]: Batch[28672/60000 (47.79%%)]  Loss: 0.0813156\n",
      "Train Epoch[2]: Batch[32768/60000 (54.61%%)]  Loss: 0.16703\n",
      "Train Epoch[2]: Batch[36864/60000 (61.44%%)]  Loss: 0.118343\n",
      "Train Epoch[2]: Batch[40960/60000 (68.27%%)]  Loss: 0.130953\n",
      "Train Epoch[2]: Batch[45056/60000 (75.09%%)]  Loss: 0.286894\n",
      "Train Epoch[2]: Batch[49152/60000 (81.92%%)]  Loss: 0.16719\n",
      "Train Epoch[2]: Batch[53248/60000 (88.75%%)]  Loss: 0.0331202\n",
      "Train Epoch[2]: Batch[57344/60000 (95.57%%)]  Loss: 0.220235\n",
      "\n",
      "Test  Epoch 2: Average loss: 0.0798143, Accuracy:97.700000%\n",
      "\n",
      "Train Epoch[3]: Batch[0/60000 (0.00%%)]  Loss: 0.133348\n",
      "Train Epoch[3]: Batch[4096/60000 (6.83%%)]  Loss: 0.197214\n",
      "Train Epoch[3]: Batch[8192/60000 (13.65%%)]  Loss: 0.248987\n",
      "Train Epoch[3]: Batch[12288/60000 (20.48%%)]  Loss: 0.0587886\n",
      "Train Epoch[3]: Batch[16384/60000 (27.31%%)]  Loss: 0.17822\n",
      "Train Epoch[3]: Batch[20480/60000 (34.13%%)]  Loss: 0.180227\n",
      "Train Epoch[3]: Batch[24576/60000 (40.96%%)]  Loss: 0.184219\n",
      "Train Epoch[3]: Batch[28672/60000 (47.79%%)]  Loss: 0.128485\n",
      "Train Epoch[3]: Batch[32768/60000 (54.61%%)]  Loss: 0.0887968\n",
      "Train Epoch[3]: Batch[36864/60000 (61.44%%)]  Loss: 0.0821515\n",
      "Train Epoch[3]: Batch[40960/60000 (68.27%%)]  Loss: 0.0643201\n",
      "Train Epoch[3]: Batch[45056/60000 (75.09%%)]  Loss: 0.141545\n",
      "Train Epoch[3]: Batch[49152/60000 (81.92%%)]  Loss: 0.113176\n",
      "Train Epoch[3]: Batch[53248/60000 (88.75%%)]  Loss: 0.0976845\n",
      "Train Epoch[3]: Batch[57344/60000 (95.57%%)]  Loss: 0.201563\n",
      "\n",
      "Test  Epoch 3: Average loss: 0.0673937, Accuracy:97.690000%\n",
      "\n",
      "Train Epoch[4]: Batch[0/60000 (0.00%%)]  Loss: 0.085938\n",
      "Train Epoch[4]: Batch[4096/60000 (6.83%%)]  Loss: 0.0945498\n",
      "Train Epoch[4]: Batch[8192/60000 (13.65%%)]  Loss: 0.187728\n",
      "Train Epoch[4]: Batch[12288/60000 (20.48%%)]  Loss: 0.0987704\n",
      "Train Epoch[4]: Batch[16384/60000 (27.31%%)]  Loss: 0.124797\n",
      "Train Epoch[4]: Batch[20480/60000 (34.13%%)]  Loss: 0.276675\n",
      "Train Epoch[4]: Batch[24576/60000 (40.96%%)]  Loss: 0.232658\n",
      "Train Epoch[4]: Batch[28672/60000 (47.79%%)]  Loss: 0.0740323\n",
      "Train Epoch[4]: Batch[32768/60000 (54.61%%)]  Loss: 0.0977014\n",
      "Train Epoch[4]: Batch[36864/60000 (61.44%%)]  Loss: 0.0352753\n",
      "Train Epoch[4]: Batch[40960/60000 (68.27%%)]  Loss: 0.0457887\n",
      "Train Epoch[4]: Batch[45056/60000 (75.09%%)]  Loss: 0.0364658\n",
      "Train Epoch[4]: Batch[49152/60000 (81.92%%)]  Loss: 0.13747\n",
      "Train Epoch[4]: Batch[53248/60000 (88.75%%)]  Loss: 0.132285\n",
      "Train Epoch[4]: Batch[57344/60000 (95.57%%)]  Loss: 0.338633\n",
      "\n",
      "Test  Epoch 4: Average loss: 0.0612537, Accuracy:97.970000%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "def train(epoch):\n",
    "    net.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(data)  # 输出结果outputs(batch_size, class_num)\n",
    "        loss = criterion(outputs, target)  \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % batch_size == 0:  # print every batch_size(64) mini-batches\n",
    "            print('Train Epoch[{}]: Batch[{}/{} ({:.2%}%)]  Loss: {:.6}'.format(\n",
    "                epoch, batch_idx * batch_size,\n",
    "                len(train_loader.dataset), batch_size * batch_idx / len(train_loader.dataset),\n",
    "                loss.data[0]\n",
    "            ))\n",
    "            train_losses.append(loss.data[0])\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    \"\"\"\n",
    "    整体思路是：feed forward, 计算出预测的结果，然后统计预测的结果中正确的所占比例即为精度，\n",
    "    在预测中不需要backward\n",
    "    :param epoch: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    test_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        outputs = net(data)\n",
    "        test_loss += criterion(outputs, target).data[0]\n",
    "        y_pred = outputs.data.max(1)[1]\n",
    "#         correct += y_pred.eq(target.data).sum()\n",
    "        correct += y_pred.eq(target.view_as(y_pred)).sum().item()\n",
    "        \n",
    "\n",
    "    avg_loss = test_loss / len(test_loader)  # loss function already averages over batch size\n",
    "    print('\\nTest  Epoch {}: Average loss: {:.6}, Accuracy:{:.6%}\\n'.format(\n",
    "        epoch, avg_loss, correct / len(test_loader.dataset)\n",
    "    ))\n",
    "\n",
    "\n",
    "for epoch in range(1, 5):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11c120a20>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1183592b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = test_set.test_data[0,:,:]\n",
    "a = torch.unsqueeze(a,0)\n",
    "a = torch.unsqueeze(a,0)\n",
    "a = a.float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sshuair/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:37: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-4436.8516, -4436.8516, -2476.6855, -2268.2866, -4436.8516,\n",
       "         -4436.8516, -4436.8516,     0.0000, -3360.7656, -3150.9802]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "net(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-c86cf63ebe5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred.eq(target.data).cpu().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(2):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 10 == 0:\n",
    "            print('[epoch{epoch}, {batch}] loss: {loss}'\n",
    "                  .format(epoch=epoch + 1, batch=i + 1, loss=running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "print('done')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just have to define the ``forward`` function, and the ``backward``\n",
    "function (where gradients are computed) is automatically defined for you\n",
    "using ``autograd``.\n",
    "You can use any of the Tensor operations in the ``forward`` function.\n",
    "\n",
    "The learnable parameters of a model are returned by ``net.parameters()``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let try a random 32x32 input\n",
    "Note: Expected input size to this net(LeNet) is 32x32. To use this net on\n",
    "MNIST dataset, please resize the images from the dataset to 32x32.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.00000e-02 *\n",
      "       [[ 5.3677,  1.8602, -0.8477, -1.8422,  5.4403,  8.7360,  5.9282,\n",
      "         -0.2987, -3.9163,  7.7429]])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero the gradient buffers of all parameters and backprops with random\n",
    "gradients:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>``torch.nn`` only supports mini-batches. The entire ``torch.nn``\n",
    "    package only supports inputs that are a mini-batch of samples, and not\n",
    "    a single sample.\n",
    "\n",
    "    For example, ``nn.Conv2d`` will take in a 4D Tensor of\n",
    "    ``nSamples x nChannels x Height x Width``.\n",
    "\n",
    "    If you have a single sample, just use ``input.unsqueeze(0)`` to add\n",
    "    a fake batch dimension.</p></div>\n",
    "\n",
    "Before proceeding further, let's recap all the classes you’ve seen so far.\n",
    "\n",
    "**Recap:**\n",
    "  -  ``torch.Tensor`` - A *multi-dimensional array* with support for autograd\n",
    "     operations like ``backward()``. Also *holds the gradient* w.r.t. the\n",
    "     tensor.\n",
    "  -  ``nn.Module`` - Neural network module. *Convenient way of\n",
    "     encapsulating parameters*, with helpers for moving them to GPU,\n",
    "     exporting, loading, etc.\n",
    "  -  ``nn.Parameter`` - A kind of Tensor, that is *automatically\n",
    "     registered as a parameter when assigned as an attribute to a*\n",
    "     ``Module``.\n",
    "  -  ``autograd.Function`` - Implements *forward and backward definitions\n",
    "     of an autograd operation*. Every ``Tensor`` operation, creates at\n",
    "     least a single ``Function`` node, that connects to functions that\n",
    "     created a ``Tensor`` and *encodes its history*.\n",
    "\n",
    "**At this point, we covered:**\n",
    "  -  Defining a neural network\n",
    "  -  Processing inputs and calling backward\n",
    "\n",
    "**Still Left:**\n",
    "  -  Computing the loss\n",
    "  -  Updating the weights of the network\n",
    "\n",
    "Loss Function\n",
    "-------------\n",
    "A loss function takes the (output, target) pair of inputs, and computes a\n",
    "value that estimates how far away the output is from the target.\n",
    "\n",
    "There are several different\n",
    "`loss functions <http://pytorch.org/docs/nn.html#loss-functions>`_ under the\n",
    "nn package .\n",
    "A simple loss is: ``nn.MSELoss`` which computes the mean-squared error\n",
    "between the input and the target.\n",
    "\n",
    "For example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(38.1824)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.arange(1, 11)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if you follow ``loss`` in the backward direction, using its\n",
    "``.grad_fn`` attribute, you will see a graph of computations that looks\n",
    "like this:\n",
    "\n",
    "::\n",
    "\n",
    "    input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
    "          -> view -> linear -> relu -> linear -> relu -> linear\n",
    "          -> MSELoss\n",
    "          -> loss\n",
    "\n",
    "So, when we call ``loss.backward()``, the whole graph is differentiated\n",
    "w.r.t. the loss, and all Tensors in the graph that has ``requres_grad=True``\n",
    "will have their ``.grad`` Tensor accumulated with the gradient.\n",
    "\n",
    "For illustration, let us follow a few steps backward:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x112238c88>\n",
      "<AddmmBackward object at 0x112238cc0>\n",
      "<ExpandBackward object at 0x10f9e6908>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backprop\n",
    "--------\n",
    "To backpropagate the error all we have to do is to ``loss.backward()``.\n",
    "You need to clear the existing gradients though, else gradients will be\n",
    "accumulated to existing gradients.\n",
    "\n",
    "\n",
    "Now we shall call ``loss.backward()``, and have a look at conv1's bias\n",
    "gradients before and after the backward.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([ 0.,  0.,  0.,  0.,  0.,  0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([-0.0296, -0.1084, -0.0346, -0.0628, -0.0655,  0.0212])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have seen how to use loss functions.\n",
    "\n",
    "**Read Later:**\n",
    "\n",
    "  The neural network package contains various modules and loss functions\n",
    "  that form the building blocks of deep neural networks. A full list with\n",
    "  documentation is `here <http://pytorch.org/docs/nn>`_.\n",
    "\n",
    "**The only thing left to learn is:**\n",
    "\n",
    "  - Updating the weights of the network\n",
    "\n",
    "Update the weights\n",
    "------------------\n",
    "The simplest update rule used in practice is the Stochastic Gradient\n",
    "Descent (SGD):\n",
    "\n",
    "     ``weight = weight - learning_rate * gradient``\n",
    "\n",
    "We can implement this using simple python code:\n",
    "\n",
    ".. code:: python\n",
    "\n",
    "    learning_rate = 0.01\n",
    "    for f in net.parameters():\n",
    "        f.data.sub_(f.grad.data * learning_rate)\n",
    "\n",
    "However, as you use neural networks, you want to use various different\n",
    "update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc.\n",
    "To enable this, we built a small package: ``torch.optim`` that\n",
    "implements all these methods. Using it is very simple:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. Note::\n",
    "\n",
    "      Observe how gradient buffers had to be manually set to zero using\n",
    "      ``optimizer.zero_grad()``. This is because gradients are accumulated\n",
    "      as explained in `Backprop`_ section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
